{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.7)\n",
      "Collecting paddlenlp\n",
      "  Downloading paddlenlp-2.5.2-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/2.3 MB\u001b[0m \u001b[31m14.4 kB/s\u001b[0m eta \u001b[36m0:01:32\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 观点抽取\n",
    "### 3.0 载入模型和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\r\n",
    "from paddlenlp.transformers import SkepForTokenClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open datasets/COTE-BD, datasets/COTE-BD.zip or datasets/COTE-BD.ZIP.\n",
      "unzip:  cannot find or open datasets/COTE-DP, datasets/COTE-DP.zip or datasets/COTE-DP.ZIP.\n",
      "unzip:  cannot find or open datasets/COTE-MFW, datasets/COTE-MFW.zip or datasets/COTE-MFW.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# 解压数据\r\n",
    "!unzip -o datasets/COTE-BD\r\n",
    "!unzip -o datasets/COTE-DP\r\n",
    "!unzip -o datasets/COTE-MFW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据内部结构解析（三个数据集的结构相同）：\n",
    "```\n",
    "train:\n",
    "label\t\ttext_a\n",
    "鸟人\t\t《鸟人》一书以鸟博士的遭遇作为主线，主要写了鸟博士从校园出来后的种种荒诞经历。\n",
    "...\t\t...\n",
    "test:\n",
    "qid\t\ttext_a\n",
    "0\t\t毕棚沟的风景早有所闻，尤其以秋季的风景最美，但是这次去晚了，红叶全掉完了，黄叶也看不到了，下了雪只...\n",
    "...\t\t..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 得到数据集字典\r\n",
    "def open_func(file_path):\r\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\r\n",
    "\r\n",
    "data_dict = {'cotebd': {'test': open_func('COTE-BD/test.tsv'),\r\n",
    "                        'train': open_func('COTE-BD/train.tsv')},\r\n",
    "             'cotedp': {'test': open_func('COTE-DP/test.tsv'),\r\n",
    "                        'train': open_func('COTE-DP/train.tsv')},\r\n",
    "             'cotemfw': {'test': open_func('COTE-MFW/test.tsv'),\r\n",
    "                        'train': open_func('COTE-MFW/train.tsv')}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 定义数据读取器\n",
    "思路类似，需要注意的是这一次是Tokens级的分类。在数据读取器中，将label写成BIO的形式，每一个token都对应一个label。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据集\r\n",
    "from paddle.io import Dataset, DataLoader\r\n",
    "from paddlenlp.data import Pad, Stack, Tuple\r\n",
    "import numpy as np\r\n",
    "label_list = {'B': 0, 'I': 1, 'O': 2}\r\n",
    "index2label = {0: 'B', 1: 'I', 2: 'O'}\r\n",
    "\r\n",
    "# 考虑token_type_id\r\n",
    "class MyDataset(Dataset):\r\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\r\n",
    "        super().__init__()\r\n",
    "        self._data = data\r\n",
    "        self._tokenizer = tokenizer\r\n",
    "        self._max_len = max_len\r\n",
    "        self._for_test = for_test\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self._data)\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        samples = self._data[idx].split('\\t')\r\n",
    "        label = samples[-2]\r\n",
    "        text = samples[-1]\r\n",
    "        if self._for_test:\r\n",
    "            origin_enc = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\r\n",
    "            return np.array(origin_enc, dtype='int64')\r\n",
    "        else:\r\n",
    "            \r\n",
    "            # 由于并不是每个字都是一个token，这里采用一种简单的处理方法，先编码label，再编码text中除了label以外的词，最后合到一起\r\n",
    "            texts = text.split(label)\r\n",
    "            label_enc = self._tokenizer.encode(label)['input_ids']\r\n",
    "            cls_enc = label_enc[0]\r\n",
    "            sep_enc = label_enc[-1]\r\n",
    "            label_enc = label_enc[1:-1]\r\n",
    "            \r\n",
    "            # 合并\r\n",
    "            origin_enc = []\r\n",
    "            label_ids = []\r\n",
    "            for index, text in enumerate(texts):\r\n",
    "                text_enc = self._tokenizer.encode(text)['input_ids']\r\n",
    "                text_enc = text_enc[1:-1]\r\n",
    "                origin_enc += text_enc\r\n",
    "                label_ids += [label_list['O']] * len(text_enc)\r\n",
    "                if index != len(texts) - 1:\r\n",
    "                    origin_enc += label_enc\r\n",
    "                    label_ids += [label_list['B']] + [label_list['I']] * (len(label_enc) - 1)\r\n",
    "\r\n",
    "            origin_enc = [cls_enc] + origin_enc + [sep_enc]\r\n",
    "            label_ids = [label_list['O']] + label_ids + [label_list['O']]\r\n",
    "            \r\n",
    "            # 截断\r\n",
    "            if len(origin_enc) > self._max_len:\r\n",
    "                origin_enc = origin_enc[:self._max_len-1] + origin_enc[-1:]\r\n",
    "                label_ids = label_ids[:self._max_len-1] + label_ids[-1:]\r\n",
    "            return np.array(origin_enc, dtype='int64'), np.array(label_ids, dtype='int64')\r\n",
    "\r\n",
    "\r\n",
    "def batchify_fn(for_test=False):\r\n",
    "    if for_test:\r\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\r\n",
    "    else:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Pad(axis=0, pad_val=label_list['O'])): [data for data in fn(samples)]\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\r\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test), shuffle=shuffle)\r\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 模型搭建并进行训练\n",
    "与之前不同的是模型换成了Token分类。由于Accuracy不再适用于Token分类，我们用Perplexity来大致衡量预测的准确度（接近1为最佳）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-25 08:47:01,638] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams and saved to /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch\n",
      "[2023-03-25 08:47:01,642] [    INFO] - Downloading skep_ernie_1.0_large_ch.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.pdparams\n",
      "100%|██████████| 1238309/1238309 [00:17<00:00, 69415.54it/s]\n",
      "W0325 08:47:19.674468    99 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0325 08:47:19.678896    99 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n",
      "[2023-03-25 08:47:27,725] [    INFO] - Downloading skep_ernie_1.0_large_ch.vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep_ernie_1.0_large_ch.vocab.txt\n",
      "100%|██████████| 55/55 [00:00<00:00, 32048.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.static import InputSpec\r\n",
    "from paddlenlp.metrics import Perplexity\r\n",
    "\r\n",
    "# 模型和分词\r\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\r\n",
    "\r\n",
    "# 参数设置\r\n",
    "data_name = 'cotedp'  # 更改此选项改变数据集\r\n",
    "\r\n",
    "## 训练相关\r\n",
    "epochs = 1\r\n",
    "learning_rate = 2e-5\r\n",
    "batch_size = 8\r\n",
    "max_len = 512\r\n",
    "\r\n",
    "## 数据相关\r\n",
    "train_dataloader = get_data_loader(data_dict[data_name]['train'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "label = InputSpec((-1, -1, 3), dtype='int64', name='label')\r\n",
    "model = paddle.Model(model, [input], [label])\r\n",
    "\r\n",
    "# 模型准备\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\r\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/1\n",
      "step  200/3158 - loss: 0.0112 - Perplexity: 1.0547 - 302ms/step\n",
      "step  400/3158 - loss: 0.0296 - Perplexity: 1.0339 - 304ms/step\n",
      "step  600/3158 - loss: 0.0074 - Perplexity: 1.0274 - 304ms/step\n",
      "step  800/3158 - loss: 0.0165 - Perplexity: 1.0240 - 300ms/step\n",
      "step 1000/3158 - loss: 0.0042 - Perplexity: 1.0215 - 301ms/step\n",
      "step 1200/3158 - loss: 0.0117 - Perplexity: 1.0198 - 300ms/step\n",
      "step 1400/3158 - loss: 0.0101 - Perplexity: 1.0183 - 301ms/step\n",
      "step 1600/3158 - loss: 0.0027 - Perplexity: 1.0174 - 301ms/step\n",
      "step 1800/3158 - loss: 0.0143 - Perplexity: 1.0166 - 301ms/step\n",
      "step 2000/3158 - loss: 0.0163 - Perplexity: 1.0159 - 301ms/step\n",
      "step 2200/3158 - loss: 0.0147 - Perplexity: 1.0155 - 301ms/step\n",
      "step 2400/3158 - loss: 0.0016 - Perplexity: 1.0149 - 301ms/step\n",
      "step 2600/3158 - loss: 0.0078 - Perplexity: 1.0145 - 301ms/step\n",
      "step 2800/3158 - loss: 0.0061 - Perplexity: 1.0141 - 302ms/step\n",
      "step 3000/3158 - loss: 0.0159 - Perplexity: 1.0137 - 301ms/step\n",
      "step 3158/3158 - loss: 0.0012 - Perplexity: 1.0135 - 301ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/0\n",
      "save checkpoint at /home/aistudio/checkpoints/final\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\r\n",
    "model.fit(train_dataloader, batch_size=batch_size, epochs=epochs, save_freq=5, save_dir='./checkpoints', log_freq=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4 预测并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-25 09:10:29,069] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n"
     ]
    }
   ],
   "source": [
    "import re\r\n",
    "\r\n",
    "# 导入预训练模型\r\n",
    "checkpoint_path = './checkpoints/final'  # 填写预训练模型的保存路径\r\n",
    "\r\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "model = paddle.Model(model, [input])\r\n",
    "model.load(checkpoint_path)\r\n",
    "\r\n",
    "# 导入测试集\r\n",
    "test_dataloader = get_data_loader(data_dict[data_name]['test'], tokenizer, batch_size, max_len, for_test=True)\r\n",
    "# 预测保存\r\n",
    "\r\n",
    "save_file = {'cotebd': './submission/COTE_BD.tsv', 'cotedp': './submission/COTE_DP.tsv', 'cotemfw': './submission/COTE_MFW.tsv'}\r\n",
    "predicts = []\r\n",
    "input_ids = []\r\n",
    "for batch in test_dataloader:\r\n",
    "    predict = model.predict_batch(batch)\r\n",
    "    predicts += predict[0].argmax(axis=-1).tolist()\r\n",
    "    input_ids += batch.numpy().tolist()\r\n",
    "\r\n",
    "# 先找到B所在的位置，即标号为0的位置，然后顺着该位置一直找到所有的I，即标号为1，即为所得。\r\n",
    "def find_entity(prediction, input_ids):\r\n",
    "    entity = []\r\n",
    "    entity_ids = []\r\n",
    "    for index, idx in enumerate(prediction):\r\n",
    "        if idx == label_list['B']:\r\n",
    "            entity_ids = [input_ids[index]]\r\n",
    "        elif idx == label_list['I']:\r\n",
    "            if entity_ids:\r\n",
    "                entity_ids.append(input_ids[index])\r\n",
    "        elif idx == label_list['O']:\r\n",
    "            if entity_ids:\r\n",
    "                entity.append(''.join(tokenizer.convert_ids_to_tokens(entity_ids)))\r\n",
    "                entity_ids = []\r\n",
    "    return entity\r\n",
    "\r\n",
    "with open(save_file[data_name], 'w', encoding='utf8') as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for idx, sample in enumerate(data_dict[data_name]['test']):\r\n",
    "        qid = sample.split('\\t')[0]\r\n",
    "        entity = find_entity(predicts[idx], input_ids[idx])\r\n",
    "        entity = list(set(entity))  # 去重\r\n",
    "        entity = [re.sub('##', '', e) for e in entity]  # 去除英文编码时的特殊符号\r\n",
    "        entity = [re.sub('[UNK]', '', e) for e in entity]  # 去除未知符号\r\n",
    "        f.write(qid + '\\t' + '\\x01'.join(entity) + '\\n')\r\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "评价对象      预测标签\n",
    "唐家三少      唐/B家/I三/I少/I，/O本/O名/O张/O威/O。/O\n",
    "加勒\t       加/B勒/I有/O用/O高/O大/O厚/O实/O的/O石/O头/O砌/O成/O的/O城/O墙/O。/O\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
