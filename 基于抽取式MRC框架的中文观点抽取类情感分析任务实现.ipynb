{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 中文观点抽取类情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "目标情感分析（Target-Based Sentiment Analysis，TBSA）是情感分析领域的一个基础问题。其目的是从一句话中识别出情感目标(或评价对象)，并预测其情感极性（积极，消极或中性）。例如，给定一句话，“重庆老灶火锅还是很赞的，有机会可以尝试一下！”模型的任务就是找出这句话描述的评价对象，并判断其情感极性。这里“重庆老灶火锅”就是评价对象（Opinion Target），它们对应的情感极性为“积极”。\n",
    "\n",
    "一个完整的目标情感分析任务，可以分为两个子任务：观点抽取（或评价对象抽取）（Opinion Target Extraction，OTE）和目标情感分类（Target Sentiment Classification，TSC）。这里我们主要研究观点抽取这个子任务。可参考千言数据集情感分析比赛[官网](https://aistudio.baidu.com/aistudio/competition/detail/50)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1 中文观点抽取任务简介\n",
    "\n",
    "观点抽取：对于给定的文本 `d`，系统需要根据文本的内容，给出其中描述的评价对象 `a`，其中评价对象 `a` 一定在文本 `d` 中出现。数据集中每个样本是一个二元组 `<d, a>`，样例如下：\n",
    "```\n",
    "输入文本（d）：重庆老灶火锅还是很赞的，有机会可以尝试一下！\n",
    "评价对象（a）：重庆老灶火锅\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2 基于抽取式MRC框架的中文观点抽取实现\n",
    "\n",
    "观点抽取任务的目标是从给定的文本中识别出其评价对象。目前主流的解决方案是基于序列标注的方法。序列标注采用某种编码方案（如BIO或IBEO等），对句子中的每一个词进行标注，比如我们采用 `BIO` 对“重庆老灶火锅还是很赞的，有机会可以尝试一下！”这句话进行标注：\n",
    "```\n",
    "tokens: ['重', '庆', '老', '灶', '火', '锅', '还', '是', '很', '赞', '的', '，', '有', '机', '会', '可', '以', '尝', '试', '一', '下', '！']\n",
    "label:  ['B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "```\n",
    "然后构建序列标注模型，预测句子中的每一个词的标记。\n",
    "\n",
    "鉴于基于指针网络的方法在抽取式机器阅读理解任务中的广泛应用，本文提出了观点抽取任务的另一种解决方案，基于抽取式 `MRC` 框架的观点抽取方法。其基本思想是将原来的序列标注任务转化为一个抽取式机器阅读理解任务去解。具体方法是，采用指针的方法对句子中的评价对象进行重新标注。观点抽取任务的目标是从一句话中抽取出这句话的评价对象，且评价对象在这句话中。我们将观点抽取任务中的句子对应到抽取式阅读理解任务中的段落，评价对象即为答案。基于抽取式 `MRC` 框架的观点抽取任务可定义为：\n",
    "```\n",
    "给定一个句子 `S` 和问题 `Q`，模型的目标是给出问题 `Q` 的答案 `A`，即评价对象，且答案 `A` 是出现在段落 `S` 中一段连续文本。\n",
    "```\n",
    "具体来说，我们需要标注出评价对象在句子中的开始位置和结束位置，如上面那个示例，评价对象“重庆老灶火锅”出现在句子中的开始位置和结束位置分别为 `0` 和 `5`。这样，我们只需要构建模型，预测评价对象出现在句子中的开始位置和结束位置，即可完成观点抽取的目标。本文实现的基于抽取式 `MRC` 框架的观点抽取模型，在测试集上取得了优于官方基于序列标注方法的 `baseline` 结果。具体地，分别在 `COTE-DP`, `COTE-BD` 和 `COTE-MFW` 三个中文观点抽取数据集的测试集上取得了 6.34%\t3.45%和1.75%的提升。\n",
    "\n",
    "本文使用的 `Pre-training + Fine-tuning` 模式的抽取式机器阅读理解架构如下，其中预训练模型使用的是 `ernie-gram-zh`："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/a479d066c6e340f2a0a28ab580dcc73393e072e51e1b462aae3aeb9cedb62c51)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本文最关键的部分是，将观点抽取数据集转换成 `SQuAD` 兼容的格式。注意，这里的 `SQuAD` 兼容格式指的不是 `SQuAD` 原始的 `json` 文件格式，而是抽取式机器阅读理解统一的输入样本格式。\n",
    "\n",
    "以 `COTE-DP`为例，`PaddleNLP` 自带的观点抽取数据集格式如下：\n",
    "```\n",
    "{\n",
    "\t'tokens': ['重', '庆', '老', '灶', '火', '锅', '还', '是', '很', '赞', '的', '，', '有', '机', '会', '可', '以', '尝', '试', '一', '下', '！'], \n",
    "    'labels': [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], \n",
    "    'entity': '重庆老灶火锅'\n",
    "}\n",
    "```\n",
    "转换成 `SQuAD` 兼容的数据格式，即抽取式 `MRC` 模型输入样本的格式如下：\n",
    "```\n",
    "{\n",
    "\t'id': 'qid0',\n",
    "    'title': '',\n",
    "    'context': '重庆老灶火锅还是很赞的，有机会可以尝试一下！',\n",
    "    'question': '评价对象',\n",
    "    'answers': ['重庆老灶火锅'],\n",
    "    'answer_starts': [0]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting paddlenlp==2.0.8\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/b0/7d/6c24cda54d018d350ee342f715523ade7871660444ed95f3d3e753d6f388/paddlenlp-2.0.8-py3-none-any.whl (571 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.7/571.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.8) (4.1.0)\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.8) (0.42.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.8) (0.70.11.1)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.8) (0.4.4)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.8) (2.9.0)\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.8) (1.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp==2.0.8) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp==2.0.8) (1.19.5)\n",
      "Requirement already satisfied: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp==2.0.8) (0.3.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.0.8) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.8) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.8) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.8) (0.14.1)\n",
      "Installing collected packages: paddlenlp\n",
      "  Attempting uninstall: paddlenlp\n",
      "    Found existing installation: paddlenlp 2.1.1\n",
      "    Uninstalling paddlenlp-2.1.1:\n",
      "      Successfully uninstalled paddlenlp-2.1.1\n",
      "Successfully installed paddlenlp-2.0.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 将 PaddleNLP 更新到最新版本\r\n",
    "!pip install paddlenlp==2.0.8 -i https://mirror.baidu.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.datasets import MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataset(data_name='dp', split='train'):\r\n",
    "    \"\"\"根据 data_name 和 split 参数创建数据集\r\n",
    "    Args:\r\n",
    "        data_name: str, 'dp', 'bd', 'mfw'\r\n",
    "        split: str, 'train', ''test\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    # 由于 COTE 数据集只提供了训练集和测试集，所以 split 参数只能是 'train' 或 'test'\r\n",
    "    assert isinstance(split, str), 'split must be str, it could be \"train\" or \"test\".'\r\n",
    "\r\n",
    "    if split == 'train':\r\n",
    "        is_test = False\r\n",
    "    elif split == 'test':\r\n",
    "        is_test = True\r\n",
    "    else:\r\n",
    "        raise ValueError('split must be \"train\" or \"test\".')\r\n",
    "\r\n",
    "    # 根据 data_name 和 split 创建数据集 (未来会从原始数据集中重新构建 SQuAD 兼容数据集)\r\n",
    "    dataset = load_dataset('cote', data_name, splits=[split], lazy=False)\r\n",
    "\r\n",
    "    # 下面我们将数据集转换成 SQuAD 兼容的格式\r\n",
    "    examples = []\r\n",
    "    for idx, example in enumerate(dataset):\r\n",
    "        qid = 'qid' + str(idx)\r\n",
    "        # tokens 对应 MRC 中的 context\r\n",
    "        context = ''.join(example['tokens'])\r\n",
    "        # 注意，原始的样本好多是以空格或NBSP字符开头，对于基于指针的方法这类位置敏感的方法而言\r\n",
    "        # 需要将开头的空格去掉\r\n",
    "        context = context.strip()\r\n",
    "        \r\n",
    "        # 原数据集里没有 question，需要我们自己设定一个。对于观点抽取任务的问题，\r\n",
    "        # 我们可以设为：'这句话的评价对象是什么？'\r\n",
    "        # 这里我简单的将问题设为：'评价对象'\r\n",
    "        # 问题的设定，对模型性能的影响，这里我没有做过多研究\r\n",
    "        # 感兴趣可以将 question 设定为一个不相干的问题试试看，\r\n",
    "        # 比如：'你吃过了吗？'\r\n",
    "        question = '评价对象'  \r\n",
    "        if not is_test:  # 训练集\r\n",
    "            answer = example['entity']\r\n",
    "\r\n",
    "            # 过滤掉没有答案的样本\r\n",
    "            answer_start = context.find(answer)\r\n",
    "            if answer_start < 0:\r\n",
    "                continue\r\n",
    "\r\n",
    "            new_example = {\r\n",
    "                'id': qid,\r\n",
    "                'title': '',\r\n",
    "                'context': context,\r\n",
    "                'question': question,\r\n",
    "                'answers': [answer],\r\n",
    "                'answer_starts': [answer_start]\r\n",
    "            }\r\n",
    "        else:  # 测试集   \r\n",
    "            new_example = {\r\n",
    "                'id': qid,\r\n",
    "                'title':'',\r\n",
    "                'context': context,\r\n",
    "                'question': question,\r\n",
    "                'answers': [],\r\n",
    "                'answer_starts': []\r\n",
    "            }\r\n",
    "\r\n",
    "        examples.append(new_example)\r\n",
    "    \r\n",
    "    # 根据样本列表创建一个 MapDataset 对象\r\n",
    "    dataset = MapDataset(examples)\r\n",
    "\r\n",
    "    # 返回数据集\r\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "我们来看一下我们创建的数据集格式是否和抽取式 `MRC` 数据集（比如，Dureader-robust）格式一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4122/4122 [00:00<00:00, 36747.96it/s]\n",
      "100%|██████████| 20038/20038 [00:00<00:00, 42477.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train_cote_dp = create_dataset('dp', split='train')\r\n",
    "train_robust = load_dataset('dureader_robust', splits='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'qid0',\n",
       " 'title': '',\n",
       " 'context': '重庆老灶火锅还是很赞的，有机会可以尝试一下！',\n",
       " 'question': '评价对象',\n",
       " 'answers': ['重庆老灶火锅'],\n",
       " 'answer_starts': [0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cote_dp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0a25cb4bc1ab6f474c699884e04601e4',\n",
       " 'title': '',\n",
       " 'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。',\n",
       " 'question': '仙剑奇侠传3第几集上天界',\n",
       " 'answers': ['第35集'],\n",
       " 'answer_starts': [0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_robust[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\r\n",
    "import math\r\n",
    "import os\r\n",
    "import random\r\n",
    "import time\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.io import DataLoader\r\n",
    "from paddle.io import BatchSampler\r\n",
    "from paddle.io import DistributedBatchSampler\r\n",
    "from paddlenlp.data import Dict\r\n",
    "from paddlenlp.data import Pad\r\n",
    "from paddlenlp.data import Stack\r\n",
    "from paddlenlp.data import Tuple\r\n",
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddlenlp.ops.optimizer import AdamW\r\n",
    "from paddlenlp.transformers import BertForQuestionAnswering\r\n",
    "from paddlenlp.transformers import BertTokenizer\r\n",
    "from paddlenlp.transformers import ErnieForQuestionAnswering\r\n",
    "from paddlenlp.transformers import ErnieTokenizer\r\n",
    "from paddlenlp.transformers import ErnieGramForQuestionAnswering\r\n",
    "from paddlenlp.transformers import ErnieGramTokenizer\r\n",
    "from paddlenlp.transformers import RobertaForQuestionAnswering\r\n",
    "from paddlenlp.transformers import RobertaTokenizer\r\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "from paddlenlp.transformers import SkepTokenizer\r\n",
    "from models import SkepForQuestionAnswering\r\n",
    "\r\n",
    "from config import Config\r\n",
    "from dataset import create_dataset\r\n",
    "from utils import CrossEntropyLossForSQuAD\r\n",
    "from utils import evaluate\r\n",
    "from utils import predict\r\n",
    "from utils import prepare_train_features\r\n",
    "from utils import prepare_validation_features\r\n",
    "from utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\r\n",
    "    \"bert\": (BertForQuestionAnswering, BertTokenizer),\r\n",
    "    \"ernie\": (ErnieForQuestionAnswering, ErnieTokenizer),\r\n",
    "    \"ernie_gram\": (ErnieGramForQuestionAnswering, ErnieGramTokenizer),\r\n",
    "    \"roberta\": (RobertaForQuestionAnswering, RobertaTokenizer),\r\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2.1 模型训与评估练代码的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_train(args):\r\n",
    "    \r\n",
    "    paddle.set_device(args.device)\r\n",
    "    set_seed(args)\r\n",
    "\r\n",
    "    # 定义分词器\r\n",
    "    args.model_type = args.model_type.lower()\r\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\r\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\r\n",
    "\r\n",
    "    # 加载数据集，并划分出验证集\r\n",
    "    train_ds = create_dataset(data_name=args.data_name, split='train')\r\n",
    "    train_ds, dev_ds = train_test_split(train_ds, test_size=0.3, random_state=args.seed)\r\n",
    "    train_ds, dev_ds = MapDataset(train_ds), MapDataset(dev_ds)\r\n",
    "\r\n",
    "    # 将数据集转化为模型输入的特征\r\n",
    "    train_trans_func = partial(\r\n",
    "        prepare_train_features, \r\n",
    "        max_seq_length=args.max_seq_length, \r\n",
    "        doc_stride=args.doc_stride,\r\n",
    "        tokenizer=tokenizer\r\n",
    "    )\r\n",
    "    train_ds.map(train_trans_func, batched=True)\r\n",
    "\r\n",
    "    dev_trans_func = partial(\r\n",
    "        prepare_validation_features, \r\n",
    "        max_seq_length=args.max_seq_length, \r\n",
    "        doc_stride=args.doc_stride,\r\n",
    "        tokenizer=tokenizer\r\n",
    "    )\r\n",
    "    dev_ds.map(dev_trans_func, batched=True)\r\n",
    "\r\n",
    "    # 定义BatchSampler\r\n",
    "    train_batch_sampler = DistributedBatchSampler(\r\n",
    "            dataset=train_ds, \r\n",
    "            batch_size=args.batch_size, \r\n",
    "            shuffle=True\r\n",
    "    )\r\n",
    "    dev_batch_sampler = BatchSampler(\r\n",
    "        dataset=dev_ds, \r\n",
    "        batch_size=args.batch_size, \r\n",
    "        shuffle=False\r\n",
    "    )\r\n",
    "    # 定义batchify_fn\r\n",
    "    train_batchify_fn = lambda samples, fn=Dict({\r\n",
    "        \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "        \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\r\n",
    "        \"start_positions\": Stack(dtype=\"int64\"),\r\n",
    "        \"end_positions\": Stack(dtype=\"int64\")\r\n",
    "    }): fn(samples)\r\n",
    "    dev_batchify_fn = lambda samples, fn=Dict({\r\n",
    "        \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "        \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\r\n",
    "    }): fn(samples)\r\n",
    "\r\n",
    "    # 构造DataLoader\r\n",
    "    train_data_loader = DataLoader(\r\n",
    "        dataset=train_ds,\r\n",
    "        batch_sampler=train_batch_sampler,\r\n",
    "        collate_fn=train_batchify_fn,\r\n",
    "        return_list=True\r\n",
    "    )\r\n",
    "    dev_data_loader =  DataLoader(\r\n",
    "        dataset=dev_ds,\r\n",
    "        batch_sampler=dev_batch_sampler,\r\n",
    "        collate_fn=dev_batchify_fn,\r\n",
    "        return_list=True\r\n",
    "    )\r\n",
    "\r\n",
    "    output_dir = os.path.join(args.output_dir, 'best_model')\r\n",
    "    if not os.path.exists(output_dir):\r\n",
    "        os.makedirs(output_dir)\r\n",
    "\r\n",
    "    # 创建模型\r\n",
    "    model = model_class.from_pretrained(args.model_name_or_path)\r\n",
    "    # model = model_class.from_pretrained(output_dir)\r\n",
    "\r\n",
    "\r\n",
    "    num_training_steps = args.max_steps if args.max_steps > 0 else len(\r\n",
    "        train_data_loader) * args.num_train_epochs\r\n",
    "    num_train_epochs = math.ceil(num_training_steps / len(train_data_loader))\r\n",
    "\r\n",
    "    num_batches = len(train_data_loader)\r\n",
    "\r\n",
    "    # 学习率调度策略 - 带学习率预热的线性衰减\r\n",
    "    lr_scheduler = LinearDecayWithWarmup(\r\n",
    "        learning_rate=args.learning_rate, \r\n",
    "        total_steps=num_training_steps,\r\n",
    "        warmup=args.warmup_proportion\r\n",
    "    )\r\n",
    "\r\n",
    "    decay_params = [\r\n",
    "        p.name for n, p in model.named_parameters()\r\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "    ]\r\n",
    "\r\n",
    "    # 定义优化器\r\n",
    "    optimizer = paddle.optimizer.AdamW(\r\n",
    "        learning_rate=lr_scheduler,\r\n",
    "        epsilon=args.adam_epsilon,\r\n",
    "        parameters=model.parameters(),\r\n",
    "        weight_decay=args.weight_decay,\r\n",
    "        apply_decay_param_fun=lambda x: x in decay_params\r\n",
    "    )\r\n",
    "\r\n",
    "    # 定义损失函数\r\n",
    "    criterion = CrossEntropyLossForSQuAD()\r\n",
    "\r\n",
    "    best_val_f1 = 0.0\r\n",
    "\r\n",
    "    global_step = 0\r\n",
    "    tic_train = time.time()\r\n",
    "\r\n",
    "    # 模型开始训练\r\n",
    "    for epoch in range(1, num_train_epochs + 1):\r\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "\r\n",
    "            global_step += 1\r\n",
    "            \r\n",
    "            input_ids, segment_ids, start_positions, end_positions = batch\r\n",
    "            logits = model(input_ids=input_ids, token_type_ids=segment_ids)\r\n",
    "            loss = criterion(logits, (start_positions, end_positions))\r\n",
    "\r\n",
    "            if global_step % args.log_steps == 0 :\r\n",
    "                # print(\"global step %d, epoch: %d, batch: %d/%d, loss: %.5f,  speed: %.2f step/s\" % (\r\n",
    "                #     global_step, epoch, step, num_batches, loss, args.log_steps / (time.time() - tic_train)))\r\n",
    "                \r\n",
    "                print(\"global step %d, epoch: %d, batch: %d/%d, loss: %.5f,  speed: %.2f step/s, lr: %1.16e\" % (\r\n",
    "                    global_step, epoch, step, num_batches, loss, args.log_steps / (time.time() - tic_train), lr_scheduler.get_lr()))\r\n",
    "                \r\n",
    "                tic_train = time.time()\r\n",
    "            \r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            lr_scheduler.step()\r\n",
    "            optimizer.clear_grad()\r\n",
    "            \r\n",
    "            # 每 save_steps 个迭代，做一次模型评估，并且模型训练结束后再做一次模型评估\r\n",
    "            if global_step % args.save_steps == 0 or global_step == num_training_steps:\r\n",
    "                em, f1 = evaluate(model=model, data_loader=dev_data_loader)\r\n",
    "\r\n",
    "                print(\"global step: %d, eval dev Exact Mactch: %.5f, f1_score: %.5f\" % (global_step, em, f1))\r\n",
    "                \r\n",
    "                # 保存 f1 值最好的模型\r\n",
    "                if f1 > best_val_f1:\r\n",
    "                    best_val_f1 = f1\r\n",
    "\r\n",
    "                    print(\"save model at global step: %d, best eval f1_score: %.5f\" % (global_step, best_val_f1))\r\n",
    "\r\n",
    "                    model.save_pretrained(output_dir)\r\n",
    "                    tokenizer.save_pretrained(output_dir)\r\n",
    "\r\n",
    "                if global_step == num_training_steps:\r\n",
    "                    break\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2.2 模型训练参数的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = Config(model_type='roberta',   # ernie_gram, skep\r\n",
    "              model_name_or_path='roberta-wwm-ext-large',   # ernie-gram-zh skep_ernie_1.0_large_ch\r\n",
    "              data_name='dp',  # dp, bd, mfw\r\n",
    "              output_dir='./outputs/cote/dp/roberta',  # './outputs/cote/{data_name}/{model_type}'\r\n",
    "              \r\n",
    "              max_seq_length=128, \r\n",
    "              batch_size=32,\r\n",
    "              learning_rate=5e-5,\r\n",
    "              num_train_epochs=10,\r\n",
    "              log_steps=10,          # dp, mfw == 20,  bd == 10\r\n",
    "              save_steps=100,        # dp, mfw == 200, bd == 100\r\n",
    "              doc_stride=64,\r\n",
    "              warmup_proportion=0.1,\r\n",
    "              weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2.2 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-02-28 10:51:50,743] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/roberta_large/vocab.txt and saved to /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large\n",
      "[2023-02-28 10:51:50,745] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/roberta_large/vocab.txt\n",
      "100%|██████████| 107/107 [00:00<00:00, 7612.81it/s]\n",
      "[2023-02-28 10:52:18,519] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/roberta_large/roberta_chn_large.pdparams and saved to /home/aistudio/.paddlenlp/models/roberta-wwm-ext-large\n",
      "[2023-02-28 10:52:18,523] [    INFO] - Downloading roberta_chn_large.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/roberta_large/roberta_chn_large.pdparams\n",
      "100%|██████████| 1271615/1271615 [07:29<00:00, 2830.22it/s] \n",
      "W0228 10:59:48.004133   144 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0228 10:59:48.012866   144 device_context.cc:422] device: 0, cuDNN Version: 8.2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step: 7120, eval dev Exact Mactch: 0.92359, f1_score: 0.96727\r"
     ]
    }
   ],
   "source": [
    "do_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.3.1 模型预测代码的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_predict(args):\r\n",
    "\r\n",
    "    paddle.set_device(args.device)\r\n",
    "\r\n",
    "    output_dir = os.path.join(args.output_dir, \"best_model\")\r\n",
    "\r\n",
    "    # 加载测试集\r\n",
    "    test_ds = create_dataset(data_name=args.data_name, split='test')\r\n",
    "\r\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\r\n",
    "    tokenizer = tokenizer_class.from_pretrained(output_dir)\r\n",
    "\r\n",
    "    # 将数据集转化为模型输入的特征\r\n",
    "    test_trans_func = partial(\r\n",
    "        prepare_validation_features, \r\n",
    "        max_seq_length=args.max_seq_length, \r\n",
    "        doc_stride=args.doc_stride,\r\n",
    "        tokenizer=tokenizer\r\n",
    "    )\r\n",
    "    test_ds.map(test_trans_func, batched=True)\r\n",
    "\r\n",
    "    # test BatchSampler\r\n",
    "    test_batch_sampler = BatchSampler(\r\n",
    "        dataset=test_ds, \r\n",
    "        batch_size=args.batch_size, \r\n",
    "        shuffle=False\r\n",
    "    )\r\n",
    "\r\n",
    "    # test dataset features batchify\r\n",
    "    test_batchify_fn = lambda samples, fn=Dict({\r\n",
    "        \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "        \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\r\n",
    "    }): fn(samples)\r\n",
    "\r\n",
    "    # test DataLoader\r\n",
    "    test_data_loader =  DataLoader(\r\n",
    "        dataset=test_ds,\r\n",
    "        batch_sampler=test_batch_sampler,\r\n",
    "        collate_fn=test_batchify_fn,\r\n",
    "        return_list=True\r\n",
    "    )\r\n",
    "\r\n",
    "    model = model_class.from_pretrained(output_dir)\r\n",
    "    \r\n",
    "    all_predictions = predict(model, test_data_loader)\r\n",
    "\r\n",
    "    # Can also write all_nbest_json and scores_diff_json files if needed\r\n",
    "    with open('COTE_' + args.data_name.upper() + '.tsv', \"w\", encoding='utf-8') as writer:\r\n",
    "        writer.write('index\\tprediction\\n')\r\n",
    "        idx = 0\r\n",
    "        for example in test_data_loader.dataset.data:\r\n",
    "            writer.write(str(idx) + '\\t' + all_predictions[example['id']] + '\\n')\r\n",
    "            idx += 1\r\n",
    "\r\n",
    "    count = 0\r\n",
    "    for example in test_data_loader.dataset.data:\r\n",
    "        count += 1\r\n",
    "        print()\r\n",
    "        print('问题：',example['question'])\r\n",
    "        print('原文：',''.join(example['context']))\r\n",
    "        print('答案：',all_predictions[example['id']])\r\n",
    "        if count >= 10:\r\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.3.2 启动模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_predict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 实验结果\n",
    "\n",
    "实验结果如下表所示：\n",
    "||DP|BD|MFW|\n",
    "|---|---|---|---|\n",
    "|Official(baseline)|0.8496|0.8649|0.8732||\n",
    "|ernie-gram|**0.913**|**0.8994**|**0.8907**|\n",
    "|diff|**0.0634**|**0.0345**|**0.0175**|\n",
    "|roberta-large|**0.9099**|**0.8917**|**0.8895**|\n",
    "|diff|**0.0603**|**0.0268**|**0.0163**|\n",
    "\n",
    "总的来看，本文提出的基于抽取 `MRC` 框架的方法在中文观点抽取的三个数据集上均取得了优于官方基于序列标注方法的 `baseline`，证明了本文方法的有效性。特别是在 `COTE-DP` 数据集上，本文提出的方法更是取得了 `6.34%` 的大幅提升。 这得益于抽取式阅读理解框架中的基于指针的标注方案。\n",
    "\n",
    "在预训练模型的选择上，本文主要使用了 `ernie-gram` 和 `roberta-large` 两个模型，其中 `ernie-gram` 为 `base` 模型。让我们很意外的是，在相同超参配置下，`large` 模型并没有取得优于 `base` 模型的效果。我们认为这可能是 `large` 模型更难优化的缘故。这里实验使用 `large` 模型主要是想和 `baseline` 做一下对比的。而 `baseline` 使用的 `skep` 模型是一个 `large` 模型，但是由于 `skep` 并不支持抽取式阅读理解任务，本文尝试对照 `PaddleNLP` 其它抽取式阅读理解模型来实现基于 `skep` 的抽取式阅读理解模型，然而实验失败了，由于时间原因，目前还没找到原因，后期有时间会继续该实验。\n",
    "\n",
    "综上，我们认为抽取式 `MRC` 框架是解决观点抽取任务的一个非常富有前景的解决方案。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3 意见反馈\n",
    "\n",
    "关于本项目有什么问题或意见可随时在NLP打卡营的 `QQ` 群里 `@我爱志方小姐`。如果您不在 `QQ` 群里里，也欢迎您在评论区留下您宝贵的建议~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
